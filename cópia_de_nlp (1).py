# -*- coding: utf-8 -*-
"""Cópia de NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XUAmF216N7uSWIe5c471-JPxqCWArDR3

# Processamento Linguagem Natural
## Analise de Reviews

Este trabalho teve como objetivo aplicar o SVM (com Bag of Words e com Embbedings) e o BERT em um banco de dados de  avaliações (reviews) e notas (ratings) do Relógio Casio F91W.

## Alunos:


*   Giovana Bueno Alves
*   Josué Jorge de Paula Oliveira
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, accuracy_score

dados = pd.read_csv('/content/final_database.csv')

dados.head()

dados.info()

dados.value_counts('rating')

dados.shape

"""Criando um dataframe apenas com as informações necessárias,
no caso o texto da review e a avaliação do produto (dado em escala de 1
a 5)
"""

df = dados.drop(columns=['Unnamed: 0','title', 'images', 'asin', 'parent_asin','user_id','timestamp','helpful_vote','verified_purchase'])

df.info()

df.head(10)

df.text[0]

"""# Transformando Texto em número"""

# Para codificação
from sklearn.feature_extraction.text import CountVectorizer

df.text

array_coment = df.text.to_numpy()

"""Verificando se possuem dados vazios ou NAN e removendo-os"""

df.isnull().sum()

df.isnull().any()

df1 = df.dropna()

df1.info()

sentimento_map = {
    1: 'uma',
    2: 'duas',
    3: 'tres',
    4: 'quatro',
    5: 'cinco'
}

# Substituindo os valores numéricos por texto na coluna 'sentimento'
df1['rating'] = df1['rating'].replace(sentimento_map)

df1.rating.head()

vetorizar = CountVectorizer(lowercase=False) #, max_features=50 #para impedir que transforme tudo em letras minusculas,
#max_features para pegar as 50 palavras que mais se repetem
bag_of_words = vetorizar.fit_transform(df1.text)

bag_of_words

bag_of_words = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorizar.get_feature_names_out())

bag_of_words

bag_of_words.shape

"""# Separado variáveis de Treinamento e Teste"""

from sklearn.model_selection import train_test_split

x_treino, x_teste, y_treino, y_teste = train_test_split(bag_of_words, df1.rating)

print("Tipo de x_treino:", type(x_treino))
print("Tipo de y_treino:", type(y_treino))

"""# Treinando o Classificador SVM + Bag of Words (BoW)"""

from sklearn.svm import SVC
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score

svm_bow = LinearSVC(random_state=42)
svm_bow.fit(x_treino, y_treino)

vectorizer = CountVectorizer()
bag_of_words = vectorizer.fit_transform(df1['text'])

# Divisão dos dados em treino e teste
x_treino, x_teste, y_treino, y_teste = train_test_split(
    bag_of_words, df1['rating'], random_state=42
)

# Verifique os tipos
print("Tipo de x_treino:", type(x_treino))  # Deve ser csr_matrix
print("Tipo de y_treino:", type(y_treino))  # Deve ser pandas Series ou array NumPy

# Treinando o modelo com LinearSVC
svm_bow = LinearSVC(random_state=42)
svm_bow.fit(x_treino, y_treino)

# Fazendo predições
y_pred_bow = svm_bow.predict(x_teste)

# Avaliando o modelo
print("Resultados com Bag of Words (BoW):")
print(classification_report(y_teste, y_pred_bow))
print("Acurácia:", accuracy_score(y_teste, y_pred_bow))

dict1 = classification_report(y_teste, y_pred_bow, output_dict=True)
# print(dict1)
accu_svm_bow = dict1['accuracy']
fs_svm_bow = dict1['weighted avg']['f1-score']
# precision_SVM_BOW, recall_SVM_BOW, fscore_SVM_BOW, sup_SVM_BOW = precision_recall_fscore_support(y_teste, y_pred_bow)

# Obter o relatório de classificação como dicionário
report_bow = classification_report(y_teste, y_pred_bow, output_dict=True)

# Converter o relatório para um DataFrame

report_df = pd.DataFrame(report_bow).transpose()

# Separar métricas por classe (ignorando 'accuracy', 'macro avg' e 'weighted avg')
classes = report_df.index[:-3]  # Pega apenas os nomes das classes
f1_scores = report_df.loc[classes, 'f1-score']
accuracies = [accuracy_score(y_teste, y_pred_bow)] * len(classes)

# Criar o gráfico de F1 e Acurácia
plt.figure(figsize=(10, 6))

# Gráfico F1
plt.bar(classes, f1_scores, alpha=0.7, label="F1-Score", color="blue")

# Gráfico de Acurácia (como linha para comparação)
plt.plot(classes, accuracies, label="Acurácia", color="orange", marker='o', linestyle='--')

# Configurações do gráfico
plt.title("F1-Score e Acurácia por Classe (SVM + Bag of Words)", fontsize=14)
plt.xlabel("Ratings", fontsize=12)
plt.ylabel("Valores", fontsize=12)
plt.xticks(rotation=45)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Exibir o gráfico
plt.show()









"""# SVM com Embeddings"""

# Pré-processamento textual antes de treinar o modelo Word2Vec
import nltk
from nltk.tokenize import word_tokenize
# tokenizador de sentença do nltk
nltk.download('punkt')
import nltk
nltk.download('punkt_tab')

tokenized_sents = [word_tokenize(i) for i in df1['text'].values]

"""# Treino do modelo Word2Vec"""

from gensim.models import Word2Vec
model_review = Word2Vec(tokenized_sents, vector_size=50, window=5, min_count=1, workers=4)

model_review

print(dir(model_review))

print(model_review.wv.key_to_index)

print(model_review.wv.get_vector('disappointed'))

"""# Mais pré-processamento..."""

# Dicionário de mapeamento
rating_map = {
    "uma": 1,
    "duas": 2,
    "tres": 3,
    "quatro": 4,
    "cinco": 5
}

from nltk.tokenize import word_tokenize
import numpy as np

X, y = [], []
max_len = 0

for i, row in df1.iterrows():
    # Tokeniza/ o texto
    tokens = word_tokenize(row['text'])

    # Gera os vetores para as palavras presentes no vocabulário do modelo
    vetores = [model_review.wv[word] for word in tokens if word in model_review.wv.key_to_index]

    # Atualiza o tamanho máximo de sequência
    max_len = max(max_len, len(vetores))

    # Converte o rating para número e adiciona às listas
    y.append(rating_map[row['rating']] - 1)  # Ajuste de índice (se necessário)
    X.append(vetores)

print("max_len: {}".format(max_len))

# Filtrar sequências vazias
X_filtered = []
y_filtered = []
for seq, label in zip(X, y):
    if len(seq) > 0:  # Apenas adiciona sequências não vazias
        X_filtered.append(seq)
        y_filtered.append(label)

# Atualizar max_len se necessário
#max_len = max(len(seq) for seq in X_filtered)

# Função de padding
def transform(exemplos, dimension):
    results = np.zeros((len(exemplos), dimension, 50))  # Assumindo dimensão 50 para vetores
    for i, sequence in enumerate(exemplos):
        results[i, :len(sequence), :] = sequence
    return results

# Aplicar padding
X_padded = transform(X_filtered, max_len)
y_array = np.array(y_filtered).astype("float32")

print("Shape de X_padded:", X_padded.shape)
print("Shape de y_array:", y_array.shape)

# Verificando o shape dos dados pré-processados (instâncias,tokens,num de dimensões do embedding)
# Converter X para um array NumPy
X_array = np.array(X_padded)

# Verificar o shape dos dados
print(X_array.shape)

"""# Split dos dados"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

"""# Definição da arquitetura da nossa rede neural"""

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

model = Sequential()
model.add(Flatten(input_shape=(max_len, 50)))
model.add(Dense(1000, activation='relu')),
model.add(Dropout(0.3))
model.add(Dense(1000, activation='relu')),
model.add(Dropout(0.3))
model.add(Dense(1000, activation='relu')),
model.add(Dense(5, activation='softmax'))
model.summary()

model.compile(
 optimizer = "adam",
 loss = "sparse_categorical_crossentropy",
 metrics = ["accuracy"]
)

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Escolha um comprimento máximo (ex.: max_len previamente calculado)
X_train = pad_sequences(X_train, maxlen=max_len, padding='post', dtype='float32')
X_test = pad_sequences(X_test, maxlen=max_len, padding='post', dtype='float32')

print("X_train shape:", X_train.shape)  # Deve ter forma consistente agora
print("X_test shape:", X_test.shape)

import numpy as np

X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

results = model.fit(
 X_train, y_train,
 epochs= 5,
 batch_size = 50,
 validation_data = (X_test, y_test)
)

print(results.history.keys())
# Output: ['loss', 'accuracy', 'val_loss', 'val_accuracy']

import matplotlib.pyplot as plt

# Plotando acurácia
plt.plot(results.history['accuracy'], label='Treinamento')
plt.plot(results.history['val_accuracy'], label='Validação')
plt.title('Acurácia por Época')
plt.xlabel('Época')
plt.ylabel('Acurácia')
plt.legend()
plt.show()

# Plotando perda
plt.plot(results.history['loss'], label='Treinamento')
plt.plot(results.history['val_loss'], label='Validação')
plt.title('Perda por Época')
plt.xlabel('Época')
plt.ylabel('Perda')
plt.legend()
plt.show()

from sklearn.metrics import f1_score

# Previsões no conjunto de teste
y_pred = model.predict(X_test)
y_pred_classes = y_pred.argmax(axis=1)

# F1-Score
f1_SVM_EMB = f1_score(y_test, y_pred_classes, average='weighted')
print(f'F1-Score no Teste: {f1_SVM_EMB}')

# Avaliando o modelo
print("Resultados com Embbeding:")
print(classification_report(y_test, y_pred_classes))
print("Acurácia:", accuracy_score(y_test, y_pred_classes))

dict_emb = classification_report(y_test, y_pred_classes, output_dict=True)
# print(dict1)
accu_svm_emb = dict_emb['accuracy']
fs_svm_emb = dict_emb['weighted avg']['f1-score']
# precision_SVM_BOW, recall_SVM_BOW, fscore_SVM_BOW, sup_SVM_BOW = precision_recall_fscore_support(y_teste, y_pred_bow)

# Obter o relatório de classificação como dicionário
report = dict_emb

# Converter o relatório para um DataFrame

report_df = pd.DataFrame(report).transpose()

# Separar métricas por classe (ignorando 'accuracy', 'macro avg' e 'weighted avg')
classes = report_df.index[:-3]  # Pega apenas os nomes das classes
f1_scores = report_df.loc[classes, 'f1-score']
accuracies = [accuracy_score(y_test, y_pred_classes)] * len(classes)

# Criar o gráfico de F1 e Acurácia
plt.figure(figsize=(10, 6))

# Gráfico F1
plt.bar(classes, f1_scores, alpha=0.7, label="F1-Score", color="blue")

# Gráfico de Acurácia (como linha para comparação)
plt.plot(classes, accuracies, label="Acurácia", color="orange", marker='o', linestyle='--')

# Configurações do gráfico
plt.title("F1-Score e Acurácia por Classe (SVM + Embbedings)", fontsize=14)
plt.xlabel("Ratings", fontsize=12)
plt.ylabel("Valores", fontsize=12)
plt.xticks(rotation=45)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Exibir o gráfico
plt.show()



"""# BERT"""

df['rating'].min()

df2  = df
df2['rating']  = df2['rating'] - 1

# Dividir os dados em treino e teste (80/20)
train_data, test_data = train_test_split(df2, test_size=0.2, random_state=42, stratify=df['rating'])
x_train = train_data['text']
y_train = train_data['rating']
x_test = test_data['text']
y_test = test_data['rating']

print("Train labels:", y_train.unique())
print("Test labels:", y_test.unique())

from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
import torch
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt

# Configurações gerais
DEVICE = torch.device("cuda")# if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")
MAX_LEN = 128
BATCH_SIZE = 8
EPOCHS = 10
LEARNING_RATE = 5e-5
NUM_LABELS = len(df['rating'].unique())  # Número de classes

# Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Dataset personalizado
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return self.texts.shape[0]

    def __getitem__(self, index):
        text = str(self.texts.iloc[index])
        label = self.labels.iloc[index]

        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(label, dtype=torch.long),
        }

# Normalizar os rótulos para começar em 0
label_min = df['rating'].min()
# train_data['rating'] = train_data['rating'] - label_min
# test_data['rating'] = test_data['rating'] - label_min

# Criar DataLoaders


train_dataset = TextDataset(x_train, y_train, tokenizer, MAX_LEN)
test_dataset = TextDataset(x_test, y_test, tokenizer, MAX_LEN)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

for batch in train_loader:
    print(batch)
    break

# Modelo
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=NUM_LABELS)
model = model.to(DEVICE)

# Otimizador
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

# Função de treinamento
def train_epoch(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0
    for batch in data_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        # if step % 10 == 0:  # A cada 10 batches
        #     print(f"Batch {step}/{len(data_loader)} processed")

    return total_loss / len(data_loader)

# Função de avaliação
def evaluate(model, data_loader, device):
    model.eval()
    preds, targets = [], []
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            targets.extend(labels.cpu().numpy())
    return accuracy_score(targets, preds), f1_score(targets, preds, average='weighted'), classification_report(targets, preds), classification_report(targets, preds, output_dict=True)

# Loop de treinamento e avaliação
train_accuracies, test_accuracies = [], []
train_f1s, test_f1s = [], []
for epoch in range(EPOCHS):
    train_loss = train_epoch(model, train_loader, optimizer, DEVICE)
    train_acc, train_f1, report_acc, dict_report_acc  = evaluate(model, train_loader, DEVICE)
    test_acc, test_f1,report, dict_report = evaluate(model, test_loader, DEVICE)

    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)
    train_f1s.append(train_f1)
    test_f1s.append(test_f1)

    print(f"Epoch {epoch + 1}:")
    print(f"  Train Loss: {train_loss:.4f}")
    print(f"  Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}")
    print(f"  Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}")


# Gráfico de comparação
plt.figure(figsize=(10, 6))
plt.plot(range(1, EPOCHS + 1), train_accuracies, label='Train Accuracy')
plt.plot(range(1, EPOCHS + 1), test_accuracies, label='Test Accuracy')
plt.plot(range(1, EPOCHS + 1), train_f1s, label='Train F1')
plt.plot(range(1, EPOCHS + 1), test_f1s, label='Test F1')
plt.xlabel("Epoch")
plt.ylabel("Score")
plt.title("BERT Performance Metrics Over Epochs")
plt.legend()
plt.show()

# Obter o relatório de classificação como dicionário
report = dict_report

# Converter o relatório para um DataFrame

report_df = pd.DataFrame(report).transpose()

# Separar métricas por classe (ignorando 'accuracy', 'macro avg' e 'weighted avg')
classes = report_df.index[:-3]  # Pega apenas os nomes das classes
f1_scores = report_df.loc[classes, 'f1-score']
accuracies = [test_acc] * len(classes)

# Criar o gráfico de F1 e Acurácia
plt.figure(figsize=(10, 6))

# Gráfico F1
plt.bar(classes, f1_scores, alpha=0.7, label="F1-Score", color="blue")

# Gráfico de Acurácia (como linha para comparação)
plt.plot(classes, accuracies, label="Acurácia", color="orange", marker='o', linestyle='--')

# Configurações do gráfico
plt.title("F1-Score e Acurácia por Classe (BERT)", fontsize=14)
plt.xlabel("Ratings", fontsize=12)
plt.ylabel("Valores", fontsize=12)
plt.xticks(rotation=45)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Exibir o gráfico
plt.show()



"""# Comparação de resultados"""

# accu_svm_bow = dict1['accuracy']
# fs_svm_bow = dict1['weighted avg']['f1-score']
# accu_svm_emb = dict1['accuracy']
# fs_svm_emb = dict1['weighted avg']['f1-score']

# Criar o gráfico de F1 e Acurácia
plt.figure(figsize=(10, 6))

# Gráfico F1
modelos = ['SVM + BoW', 'SVM + Embbeding', 'BERT']
f1_score = [fs_svm_bow, fs_svm_bow, test_f1]
accu = [accu_svm_bow, accu_svm_emb, test_acc]
plt.bar(modelos, f1_score, alpha=0.7, label="F1-Score", color="blue")

# Gráfico de Acurácia (como linha para comparação)
# plt.plot(classes, accu, label="Acurácia", color="orange", marker='o', linestyle='--')

# Configurações do gráfico
plt.title("F1-Score por Modelo", fontsize=14)
plt.xlabel("F1-Score", fontsize=12)
plt.ylabel("Modelo", fontsize=12)
plt.xticks(rotation=45)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Exibir o gráfico
plt.show()

# accu_svm_bow = dict1['accuracy']
# fs_svm_bow = dict1['weighted avg']['f1-score']
# accu_svm_emb = dict1['accuracy']
# fs_svm_emb = dict1['weighted avg']['f1-score']

# Criar o gráfico de F1 e Acurácia
plt.figure(figsize=(10, 6))

# Gráfico F1
modelos = ['SVM + BoW', 'SVM + Embbeding', 'BERT']
# f1_score = [fs_svm_bow, fs_svm_bow, test_f1]
# accu = [accu_svm_bow, accu_svm_emb, test_acc]
plt.bar(modelos, accu, alpha=0.7, label="F1-Score", color="blue")

# Gráfico de Acurácia (como linha para comparação)
# plt.plot(classes, accu, label="Acurácia", color="orange", marker='o', linestyle='--')

# Configurações do gráfico
plt.title("Acurácia por Modelo", fontsize=14)
plt.xlabel("Acurácia", fontsize=12)
plt.ylabel("Modelo", fontsize=12)
plt.xticks(rotation=45)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Exibir o gráfico
plt.show()

# EMBBEDING
report_emb= dict_emb
report_emb_df = pd.DataFrame(report_emb).transpose()
classes_emb = report_emb_df.index[:-3]  # Pega apenas os nomes das classes
f1_scores_emb = report_emb_df.loc[classes_emb, 'f1-score']
accuracies_emb = [dict_emb['accuracy']] * len(classes_emb)


# BOW
# report_bow = classification_report(y_teste, y_pred_bow, output_dict=True)
report_bow_df = pd.DataFrame(report_bow).transpose()
classes_bow = report_bow_df.index[:-3]  # Pega apenas os nomes das classes
f1_scores_bow = report_bow_df.loc[classes_bow, 'f1-score']
accuracies_bow = [accuracy_score(y_teste, y_pred_bow)] * len(classes_bow)

# BERT
report_bert = dict_report
report_bert_df = pd.DataFrame(report_bert).transpose()
classes_bert = report_bert_df.index[:-3]  # Pega apenas os nomes das classes
f1_scores_bert = report_bert_df.loc[classes_bert, 'f1-score']
accuracies_bert = [test_acc] * len(classes_bert)



# Classes para o eixo X
classes = [0,1,2,3,4]

# Criar o gráfico
plt.figure(figsize=(12, 8))

# F1-Scores para cada modelo
width = 0.2  # Largura das barras
x = np.arange(len(classes))  # Posições das classes no eixo X

# Barras de F1-Score
plt.bar(x - width, f1_scores_emb, width, label="F1-Score - SVM + Embbeding", color="blue", alpha=0.7)
plt.bar(x, f1_scores_bow, width, label="F1-Score - SVM + BoW", color="green", alpha=0.7)
plt.bar(x + width, f1_scores_bert, width, label="F1-Score - BERT", color="red", alpha=0.7)

# Linhas de Acurácia
plt.plot(x - width, accuracies_emb, label="Acurácia - SVM + Embbeding", color="blue", marker="o", linestyle="--")
plt.plot(x, accuracies_bow, label="Acurácia - SVM + BoW", color="green", marker="o", linestyle="--")
plt.plot(x + width, accuracies_bert, label="Acurácia - BERT", color="red", marker="o", linestyle="--")

# Configurações do gráfico
plt.title("Comparação de F1-Score e Acurácia por Classe para 3 Modelos", fontsize=14)
plt.xlabel("Classes", fontsize=12)
plt.ylabel("Valores", fontsize=12)
plt.xticks(x, classes)  # Configura as posições das classes no eixo X
plt.legend(fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()

# Exibir o gráfico
plt.show()

"""# Parei por aqui"""

import numpy as np
from gensim.models import KeyedVectors

# Carregar embeddings pré-treinados (exemplo com GloVe ou Word2Vec)
# Substitua 'word2vec_path' pelo caminho do modelo pré-treinado
word2vec = KeyedVectors.load_word2vec_format('word2vec_path', binary=True)

# Função para converter texto em embeddings
def text_to_embedding(texto, model):
    words = texto.split()
    embeddings = [model[word] for word in words if word in model]
    if embeddings:
        return np.mean(embeddings, axis=0)
    else:
        return np.zeros(model.vector_size)

# Aplicar nos dados
x_treino_embeddings = np.array([text_to_embedding(texto, word2vec) for texto in x_treino])
x_teste_embeddings = np.array([text_to_embedding(texto, word2vec) for texto in x_teste])

# Treinando SVM com embeddings
svm_embeddings = SVC(kernel='linear', random_state=42)
svm_embeddings.fit(x_treino_embeddings, y_treino)

# Fazendo predições
y_pred_embeddings = svm_embeddings.predict(x_teste_embeddings)

# Avaliando o modelo
print("Resultados com Embeddings:")
print(classification_report(y_teste, y_pred_embeddings))
print("Acurácia:", accuracy_score(y_teste, y_pred_embeddings))

















# Utilizando dataset de notícias do Sklearn
from sklearn.datasets import fetch_20newsgroups

# Representação de palavras

# Para codificação one-hot encoder
from sklearn.preprocessing import OneHotEncoder

# Para codificação Bag-of-words
from sklearn.feature_extraction.text import CountVectorizer

# Para codificação TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

# Modelos
from	sklearn.naive_bayes	import	MultinomialNB

# métricas
from sklearn import metrics
from sklearn.metrics import confusion_matrix

df1.rating.head()

x_treino, x_teste, y_treino, y_teste = train_test_split(bag_of_words, df1.rating, random_state=42)

print(x_treino.shape[0])
print(x_teste.shape[0])
print(y_treino.shape[0])
print(y_teste.shape[0])

from sklearn.linear_model import LogisticRegression

regressao_logistica = LogisticRegression()
regressao_logistica.fit(x_treino, y_treino)
acuracia = regressao_logistica.score(x_teste, y_teste)
print(acuracia)

"""# Visualizar Palavras mais frequentes"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

todas_palavras = [texto for texto in df1.text]

todas_palavras

"""# Até aqui ele tem os comentários separados por vírgula, cada um em uma linha"""

df1.head()

todas_palavras = ' '.join([texto for texto in df1.text])

todas_palavras

nuvem_palavras = WordCloud().generate(todas_palavras)

plt.figure
plt.imshow(nuvem_palavras)
plt.axis('off')

nuvem_palavras = WordCloud(background_color='white', width=1000, height=1000, max_font_size=110, collocations=False).generate(todas_palavras)

plt.figure(figsize=(10,10))
plt.imshow(nuvem_palavras, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Separando por sentimento"""

def nuvem_palavras(texto, coluna_texto, sentimento):
  if sentimento not in texto['rating'].values:
        print(f"Sentimento '{sentimento}' não encontrado! Verifique se o valor está correto.")
        return
  # Filtrando as resenhas com base no sentimento especificado
  texto_sentimento = texto.query(f"rating == '{sentimento}'")[coluna_texto]

  # Unindo todas as resenhas em uma única string
  texto_unido = ' '.join(texto_sentimento)

  # Criando e exibindo a nuvem de palavras
  nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(texto_unido)
  plt.figure(figsize=(10,7))
  plt.imshow(nuvem_palavras, interpolation='bilinear')
  plt.axis('off')
  plt.show()

"""# Palavras mais frequentes para avaliações positivas"""

nuvem_palavras(df1, 'text', 'cinco')

"""# Palavras mais frequentes para avaliações negativas"""

nuvem_palavras(df1, 'text', 'uma')

"""# Frequencia das palavras"""

import nltk

nltk.download('all')

frases = ['love' , 'never']
frequencia = nltk.FreqDist(frases)
frequencia

from nltk import tokenize

frase = 'i love it i think is comfortable'
token_espaco = tokenize.WhitespaceTokenizer()
token_frase = token_espaco.tokenize(frase)
print(token_frase)

token_frase = token_espaco.tokenize(todas_palavras)

token_frase

frequencia = nltk.FreqDist(token_frase)
frequencia

df_frequencia = pd.DataFrame({'Palavras': list(frequencia.keys()), 'Frequencia': list(frequencia.values())})

df_frequencia

df_frequencia.nlargest(columns='Frequencia', n=10)

import seaborn as sns

plt.figure(figsize=(10,8))
ax = sns.barplot(x='Palavras', y='Frequencia', color = 'gray', data=df_frequencia.nlargest(columns='Frequencia', n=20))
ax.set(ylabel='Contagem')
plt.show()